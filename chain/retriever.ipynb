{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 0}, page_content='Multivariate Statistics\\nNeural Networks and Deep Learning\\nOctober 17, 2024'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 1}, page_content='Neural Network Data\\n•The building block of deep learning is the neural network\\n•A neural network takes a matrix input X of p predictors \\n(commonly called “features” in this context)\\n•Even images, videos and other complex objects can be \\nrepresented as a data matrix (if it is on a computer, it has \\nsome numeric representation after all)\\n•We’ll discuss a simple “single layer feed -forward neural \\nnetwork” first'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 2}, page_content='Feed -Forward Neural Network I\\n•Suppose we have a quantitative response Y, and three \\npredictors (p = 3)\\n•The data from these 3 predictors makes up the “input layer” \\nof a neural network \\n•Each of the inputs from this “input layer” feed into K “hidden \\nunits” that make up the “hidden layer” \\n•We choose the number of “hidden units” and the number of \\n“hidden layers” (these are hyper -parameters)'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 3}, page_content='Feed -Forward Neural Network II\\n•The hidden layer is used to transform the data non -linearly \\nusing activation functions\\n•The non -linear transformation is particularly important; it \\nmeans the hidden layer is responsible for learning complex \\nfeatures for the data\\n•These K activations (one for each unit in the layer) will then \\nfeed into the output layer\\n•The output layer simply outputs our prediction'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 4}, page_content='Neural Network Diagram\\n•The circles represent the \\nnodes or units in each layer\\n•We have three inputs (p=3)\\n•We have a single hidden layer\\n•K = 4 units in the hidden layer\\n•It is “fully connected” because \\neach node is connected to \\nevery node in the next layer\\n'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 5}, page_content='Hidden Layer\\n•Again, suppose we have p = 3 and 4 hidden layer nodes\\n•The layer takes the data and uses all the features as inputs \\nto a non -linear activation function which is chosen by us\\n•Mathematically: 𝐴𝑘=𝑔(𝑤𝑘0+ 𝑤𝑘1𝑋1+ 𝑤𝑘2𝑋2+𝑤𝑘3𝑋3)\\n•The k is the individual node (here it can be 1, 2, 3, or 4)\\n•The function g(.) is the activation function\\n•The 𝑤𝑘𝑗 are the “weights” which will be estimated from the \\ndata (notice the similarity here to OLS)'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 6}, page_content='Activation Function\\n•We choose the activation function ourself and there are \\nmany possible choices that have a huge impact on results\\n•A few well -known choices: ReLU , logistic, Tanh\\n•These add a “non -linear” component to the neural network\\n•The functions take a linear input (the weights and features of \\nthe data for our first hidden layer) and return something else\\n•Whatever they return is passed on to the output layer'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 7}, page_content='ReLU  Function\\n•Define ReLU  as:  g 𝑧=ቊ0,𝑧<0\\n𝑧,𝑧≥0\\n•In words: the function takes an input z, it returns 0 if this \\ninput’s value is a negative number\\n•It returns the input itself (z) if it is zero or positive\\n•In our example: 𝑧= 𝑤𝑘0+ 𝑤𝑘1𝑋1+ 𝑤𝑘2𝑋2+𝑤𝑘3𝑋3\\n•So, it will return 0, or 𝑤𝑘0+ 𝑤𝑘1𝑋1+ 𝑤𝑘2𝑋2+𝑤𝑘3𝑋3 '),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 8}, page_content='ReLU  Function Graph\\n•Notice that at z = 0, the \\nfunction changes\\n•If z < 0 we have a flat line \\nequal to zero \\n•If z is 0 or larger, we have \\nf(z)=z\\n•We’ll discuss why this function \\nworks well later\\n'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 9}, page_content='Output Layer: Regression\\n•As discussed before, the output layer receives it’s “input” \\nfrom the hidden layer, not the input layer)\\n•In our example, let there be a single node in the output layer\\n•It receives inputs from all 4 nodes in the hidden layer\\n•If we expect an output like a linear regression (i.e. a \\npredicted value for sales) then output is a linear function \\n•output = β0+β1𝐴1+ β2𝐴2+β3𝐴3+β4𝐴4\\n•We have 4 A because we have 4 nodes in the hidden layer'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 10}, page_content='Output Layer: Classification\\n•A linear output layer will not work for classification \\n•We discussed this before: we need something that outputs a \\nprobability between 0 and 1\\n•The softmax  function is used here: prob(Y = m) = 𝑒𝑍𝑚\\nσ𝑖=0𝐽𝑒𝑍𝑖\\n•Here we have J different classes (for example if we the \\nresponse can be COVID, FLU, COLD then J = 3)\\n•Formula is the probability the response is a particular class'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 11}, page_content='Multilayer Neural Networks\\n•The term “deep learning” refers to the addition of many \\nlayers to the network\\n•As a simple extension, let’s consider two hidden layers \\ninstead of one\\n•Each hidden layer functions exactly the same as we \\ndescribed before\\n•The difference is that the second hidden layer gets its input \\nfrom the first hidden layer (not the input layer)'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 12}, page_content='Multilayer Neural Network: Diagram\\n'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 13}, page_content='Multilayer Neural Mathematics\\n•Assume 3 inputs and 3 nodes for each hidden layer\\n•Now each A is indexed by its position in the hidden layer as \\nwell as which layer it belongs to 𝐴𝑛𝑜𝑑𝑒  𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛  𝑖𝑛 𝑙𝑎𝑦𝑒𝑟(𝐻𝑖𝑑𝑑𝑒𝑛  𝐿𝑎𝑦𝑒𝑟  #)\\n•𝐴𝑘(1)=𝑔(𝑤𝑘0(1)+𝑤𝑘1(1)𝑋1+𝑤𝑘2(1)𝑋2+𝑤𝑘3(1)𝑋3)\\n•𝐴𝑙(2)=𝑔(𝑤𝑙0(2)+𝑤𝑙1(2)𝐴1(1)+𝑤𝑙2(2)𝐴2(1)+𝑤𝑙3(2)𝐴3(1))\\n•Regression Output (Linear) = β0+β1𝐴1(2)+β2𝐴2(2)+β3𝐴3(2)'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 14}, page_content='Fitting the Neural Network\\n•Recall: The Linear Regression model estimated its \\nparameters by minimizing the sum of squared residuals\\n•The Neural Network also must estimate its parameters: both \\nin the input layer and the hidden layer (the 𝑤𝑘𝑖 and β𝑖)\\n•How it does this is by far the most complicated part of the \\nprocess\\n•Our software will handle this part for us, but we’ll briefly \\ndiscuss some of the key terminology'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 15}, page_content='The Loss Function\\n•First, we need some function which we seek to minimize, we \\noften call this the “Loss function”\\n•For regression problems, this is often the MSE (mean \\nsquared error): 1\\n𝑛σ𝑖=0𝑛(𝑦𝑖−ෝ𝑦𝑖)2\\n•For classification problems, this is often “cross -entropy loss”\\n•An algorithm family called “Gradient Descent” is used to \\nminimize this due to thecomplexity  in neural networks'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 16}, page_content='Gradient Descent: Intuition\\n•You can visualize the algorithm: imagine it is completely \\nblack outside and you are standing near a valley and trying \\nto get to the lowest point \\n•You have no idea where the lowest point is and cannot see it\\n•At each step, you put one foot forward and test the ground in \\nfront of you by moving your foot around\\n•You move in the direction that your foot feels is “lowest”\\n•You do not move if your foot feels only “higher” or the same'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 17}, page_content='Gradient Descent: Loss Function\\n•This roughly describes how the gradient descent algorithm \\nworks\\n•At each “step” it tests the parameters by slightly modifying \\nthem (like we test the ground by slightly moving our foot)\\n•If we find parameter values that “lower” the value of the loss \\nfunction we continue in that direction\\n•We proceed until we cannot lower the loss any further, only \\nraise it or keep it the same'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 18}, page_content='Backpropagation\\n•The basic idea here is that a neural network can have many \\nlayers (but the output layer which minimizes the cost only \\ndirectly has the parameters from the last layer)\\n•However all parameters from previous layers are indirectly \\nthere and by applying the chain rule from calculus, results \\ncan “flow” backwards from the output layer to all others\\n•This means that the backpropagation algorithm efficiently \\nupdates the parameters for all layers'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 19}, page_content='Special Types of Neural Networks\\n•There are many special configurations (layer choices, \\nactivation functions etc) that solve particular problems\\n•For example, the CNN (convolutional neural network) is \\nused on problems in computer vision (image classification)\\n•The RNN (recurrent neural network) is used whenever the \\n“order” of the data has special meaning\\n•For example, RNN is used in natural language processing \\nbecause the order of words matters '),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 20}, page_content='Benefits/Drawbacks of Neural Networks\\n•Neural networks and deep learning have become extremely \\nsuccessful in two fields: image classification and \\nspeech/language tasks\\n•When large amounts of data are available, they generally \\noutperform other models\\n•However they have a major flaw: they are a “black box” \\ntechnique; hard to interpret and understand\\n•It is typically unclear exactly how it makes predictions'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 21}, page_content='Next Steps\\n•With this brief overview of deep learning, you should be \\nprepared for further self -study on the topic\\n•In the course, we’ll move on to high dimensional data\\n•The topic of regularization will come up soon, which is \\nextremely useful in a deep learning context since there are \\nmany parameters in these models'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 22}, page_content='Summary\\n•Neural Networks consist of special layers of nodes, where each \\nnode has its own parameters (often called weights)\\n•The nodes take inputs from the previous layer (the input layer \\ntakes its inputs from the data)\\n•The hidden layers modify the input using some non -linear \\nactivation function\\n•The output layer returns a number (for regression) or a class (for \\nclassification)\\n    ')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf\")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 0}, page_content='Multivariate Statistics\\nNeural Networks and Deep Learning\\nOctober 17, 2024'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 1}, page_content='Neural Network Data\\n•The building block of deep learning is the neural network\\n•A neural network takes a matrix input X of p predictors \\n(commonly called “features” in this context)\\n•Even images, videos and other complex objects can be \\nrepresented as a data matrix (if it is on a computer, it has \\nsome numeric representation after all)\\n•We’ll discuss a simple “single layer feed -forward neural \\nnetwork” first'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 2}, page_content='Feed -Forward Neural Network I\\n•Suppose we have a quantitative response Y, and three \\npredictors (p = 3)\\n•The data from these 3 predictors makes up the “input layer” \\nof a neural network \\n•Each of the inputs from this “input layer” feed into K “hidden \\nunits” that make up the “hidden layer” \\n•We choose the number of “hidden units” and the number of \\n“hidden layers” (these are hyper -parameters)'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 3}, page_content='Feed -Forward Neural Network II\\n•The hidden layer is used to transform the data non -linearly \\nusing activation functions\\n•The non -linear transformation is particularly important; it \\nmeans the hidden layer is responsible for learning complex \\nfeatures for the data\\n•These K activations (one for each unit in the layer) will then \\nfeed into the output layer\\n•The output layer simply outputs our prediction'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 4}, page_content='Neural Network Diagram\\n•The circles represent the \\nnodes or units in each layer\\n•We have three inputs (p=3)\\n•We have a single hidden layer\\n•K = 4 units in the hidden layer\\n•It is “fully connected” because \\neach node is connected to \\nevery node in the next layer')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "text_splitter.split_documents(docs)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 0}, page_content='Multivariate Statistics\\nNeural Networks and Deep Learning\\nOctober 17, 2024'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 1}, page_content='Neural Network Data\\n•The building block of deep learning is the neural network\\n•A neural network takes a matrix input X of p predictors \\n(commonly called “features” in this context)\\n•Even images, videos and other complex objects can be \\nrepresented as a data matrix (if it is on a computer, it has \\nsome numeric representation after all)\\n•We’ll discuss a simple “single layer feed -forward neural \\nnetwork” first'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 2}, page_content='Feed -Forward Neural Network I\\n•Suppose we have a quantitative response Y, and three \\npredictors (p = 3)\\n•The data from these 3 predictors makes up the “input layer” \\nof a neural network \\n•Each of the inputs from this “input layer” feed into K “hidden \\nunits” that make up the “hidden layer” \\n•We choose the number of “hidden units” and the number of \\n“hidden layers” (these are hyper -parameters)'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 3}, page_content='Feed -Forward Neural Network II\\n•The hidden layer is used to transform the data non -linearly \\nusing activation functions\\n•The non -linear transformation is particularly important; it \\nmeans the hidden layer is responsible for learning complex \\nfeatures for the data\\n•These K activations (one for each unit in the layer) will then \\nfeed into the output layer\\n•The output layer simply outputs our prediction'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 4}, page_content='Neural Network Diagram\\n•The circles represent the \\nnodes or units in each layer\\n•We have three inputs (p=3)\\n•We have a single hidden layer\\n•K = 4 units in the hidden layer\\n•It is “fully connected” because \\neach node is connected to \\nevery node in the next layer'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 5}, page_content='Hidden Layer\\n•Again, suppose we have p = 3 and 4 hidden layer nodes\\n•The layer takes the data and uses all the features as inputs \\nto a non -linear activation function which is chosen by us\\n•Mathematically: 𝐴𝑘=𝑔(𝑤𝑘0+ 𝑤𝑘1𝑋1+ 𝑤𝑘2𝑋2+𝑤𝑘3𝑋3)\\n•The k is the individual node (here it can be 1, 2, 3, or 4)\\n•The function g(.) is the activation function\\n•The 𝑤𝑘𝑗 are the “weights” which will be estimated from the \\ndata (notice the similarity here to OLS)'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 6}, page_content='Activation Function\\n•We choose the activation function ourself and there are \\nmany possible choices that have a huge impact on results\\n•A few well -known choices: ReLU , logistic, Tanh\\n•These add a “non -linear” component to the neural network\\n•The functions take a linear input (the weights and features of \\nthe data for our first hidden layer) and return something else\\n•Whatever they return is passed on to the output layer'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 7}, page_content='ReLU  Function\\n•Define ReLU  as:  g 𝑧=ቊ0,𝑧<0\\n𝑧,𝑧≥0\\n•In words: the function takes an input z, it returns 0 if this \\ninput’s value is a negative number\\n•It returns the input itself (z) if it is zero or positive\\n•In our example: 𝑧= 𝑤𝑘0+ 𝑤𝑘1𝑋1+ 𝑤𝑘2𝑋2+𝑤𝑘3𝑋3\\n•So, it will return 0, or 𝑤𝑘0+ 𝑤𝑘1𝑋1+ 𝑤𝑘2𝑋2+𝑤𝑘3𝑋3'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 8}, page_content='ReLU  Function Graph\\n•Notice that at z = 0, the \\nfunction changes\\n•If z < 0 we have a flat line \\nequal to zero \\n•If z is 0 or larger, we have \\nf(z)=z\\n•We’ll discuss why this function \\nworks well later'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 9}, page_content='Output Layer: Regression\\n•As discussed before, the output layer receives it’s “input” \\nfrom the hidden layer, not the input layer)\\n•In our example, let there be a single node in the output layer\\n•It receives inputs from all 4 nodes in the hidden layer\\n•If we expect an output like a linear regression (i.e. a \\npredicted value for sales) then output is a linear function \\n•output = β0+β1𝐴1+ β2𝐴2+β3𝐴3+β4𝐴4\\n•We have 4 A because we have 4 nodes in the hidden layer'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 10}, page_content='Output Layer: Classification\\n•A linear output layer will not work for classification \\n•We discussed this before: we need something that outputs a \\nprobability between 0 and 1\\n•The softmax  function is used here: prob(Y = m) = 𝑒𝑍𝑚\\nσ𝑖=0𝐽𝑒𝑍𝑖\\n•Here we have J different classes (for example if we the \\nresponse can be COVID, FLU, COLD then J = 3)\\n•Formula is the probability the response is a particular class'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 11}, page_content='Multilayer Neural Networks\\n•The term “deep learning” refers to the addition of many \\nlayers to the network\\n•As a simple extension, let’s consider two hidden layers \\ninstead of one\\n•Each hidden layer functions exactly the same as we \\ndescribed before\\n•The difference is that the second hidden layer gets its input \\nfrom the first hidden layer (not the input layer)'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 12}, page_content='Multilayer Neural Network: Diagram'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 13}, page_content='Multilayer Neural Mathematics\\n•Assume 3 inputs and 3 nodes for each hidden layer\\n•Now each A is indexed by its position in the hidden layer as \\nwell as which layer it belongs to 𝐴𝑛𝑜𝑑𝑒  𝑝𝑜𝑠𝑖𝑡𝑖𝑜𝑛  𝑖𝑛 𝑙𝑎𝑦𝑒𝑟(𝐻𝑖𝑑𝑑𝑒𝑛  𝐿𝑎𝑦𝑒𝑟  #)\\n•𝐴𝑘(1)=𝑔(𝑤𝑘0(1)+𝑤𝑘1(1)𝑋1+𝑤𝑘2(1)𝑋2+𝑤𝑘3(1)𝑋3)\\n•𝐴𝑙(2)=𝑔(𝑤𝑙0(2)+𝑤𝑙1(2)𝐴1(1)+𝑤𝑙2(2)𝐴2(1)+𝑤𝑙3(2)𝐴3(1))\\n•Regression Output (Linear) = β0+β1𝐴1(2)+β2𝐴2(2)+β3𝐴3(2)'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 14}, page_content='Fitting the Neural Network\\n•Recall: The Linear Regression model estimated its \\nparameters by minimizing the sum of squared residuals\\n•The Neural Network also must estimate its parameters: both \\nin the input layer and the hidden layer (the 𝑤𝑘𝑖 and β𝑖)\\n•How it does this is by far the most complicated part of the \\nprocess\\n•Our software will handle this part for us, but we’ll briefly \\ndiscuss some of the key terminology'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 15}, page_content='The Loss Function\\n•First, we need some function which we seek to minimize, we \\noften call this the “Loss function”\\n•For regression problems, this is often the MSE (mean \\nsquared error): 1\\n𝑛σ𝑖=0𝑛(𝑦𝑖−ෝ𝑦𝑖)2\\n•For classification problems, this is often “cross -entropy loss”\\n•An algorithm family called “Gradient Descent” is used to \\nminimize this due to thecomplexity  in neural networks'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 16}, page_content='Gradient Descent: Intuition\\n•You can visualize the algorithm: imagine it is completely \\nblack outside and you are standing near a valley and trying \\nto get to the lowest point \\n•You have no idea where the lowest point is and cannot see it\\n•At each step, you put one foot forward and test the ground in \\nfront of you by moving your foot around\\n•You move in the direction that your foot feels is “lowest”\\n•You do not move if your foot feels only “higher” or the same'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 17}, page_content='Gradient Descent: Loss Function\\n•This roughly describes how the gradient descent algorithm \\nworks\\n•At each “step” it tests the parameters by slightly modifying \\nthem (like we test the ground by slightly moving our foot)\\n•If we find parameter values that “lower” the value of the loss \\nfunction we continue in that direction\\n•We proceed until we cannot lower the loss any further, only \\nraise it or keep it the same'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 18}, page_content='Backpropagation\\n•The basic idea here is that a neural network can have many \\nlayers (but the output layer which minimizes the cost only \\ndirectly has the parameters from the last layer)\\n•However all parameters from previous layers are indirectly \\nthere and by applying the chain rule from calculus, results \\ncan “flow” backwards from the output layer to all others\\n•This means that the backpropagation algorithm efficiently \\nupdates the parameters for all layers'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 19}, page_content='Special Types of Neural Networks\\n•There are many special configurations (layer choices, \\nactivation functions etc) that solve particular problems\\n•For example, the CNN (convolutional neural network) is \\nused on problems in computer vision (image classification)\\n•The RNN (recurrent neural network) is used whenever the \\n“order” of the data has special meaning\\n•For example, RNN is used in natural language processing \\nbecause the order of words matters'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 20}, page_content='Benefits/Drawbacks of Neural Networks\\n•Neural networks and deep learning have become extremely \\nsuccessful in two fields: image classification and \\nspeech/language tasks\\n•When large amounts of data are available, they generally \\noutperform other models\\n•However they have a major flaw: they are a “black box” \\ntechnique; hard to interpret and understand\\n•It is typically unclear exactly how it makes predictions'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 21}, page_content='Next Steps\\n•With this brief overview of deep learning, you should be \\nprepared for further self -study on the topic\\n•In the course, we’ll move on to high dimensional data\\n•The topic of regularization will come up soon, which is \\nextremely useful in a deep learning context since there are \\nmany parameters in these models'),\n",
       " Document(metadata={'source': 'C://Users//erons//Documents//Predictive Analytics//Multivariate Statistics//SLides//Week 11 DeepLearning.pdf', 'page': 22}, page_content='Summary\\n•Neural Networks consist of special layers of nodes, where each \\nnode has its own parameters (often called weights)\\n•The nodes take inputs from the previous layer (the input layer \\ntakes its inputs from the data)\\n•The hidden layers modify the input using some non -linear \\nactivation function\\n•The output layer returns a number (for regression) or a class (for \\nclassification)')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents=text_splitter.split_documents(docs)\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "db=FAISS.from_documents(documents[:30],OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x21c06a523b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Summary\\n•Neural Networks consist of special layers of nodes, where each \\nnode has its own parameters (often called weights)\\n•The nodes take inputs from the previous layer (the input layer \\ntakes its inputs from the data)\\n•The hidden layers modify the input using some non -linear \\nactivation function\\n•The output layer returns a number (for regression) or a class (for \\nclassification)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query=\"What are neural networks\"\n",
    "result=db.similarity_search(query)\n",
    "result[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "## Load Ollama LAMA2 LLM model\n",
    "llm=Ollama(model=\"llama2\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Design ChatPrompt Template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based only on the provided context. \n",
    "Think step by step before providing a detailed answer. \n",
    "I will reward you if the user finds the answer helpful. \n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chain Introduction\n",
    "## Create Stuff Docment Chain\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain=create_stuff_documents_chain(llm,prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x0000021C06A523B0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever=db.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Retrieval chain:This chain takes in a user inquiry, which is then\n",
    "passed to the retriever to fetch relevant documents. Those documents \n",
    "(and original inputs) are then passed to an LLM to generate a response\n",
    "https://python.langchain.com/docs/modules/chains/\n",
    "\"\"\"\n",
    "from langchain.chains import create_retrieval_chain\n",
    "retrieval_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=retrieval_chain.invoke({\"input\":\"Hidden Layer\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Great, let's dive into the details of the hidden layer in a multilayer neural network.\\n\\nThe hidden layer is responsible for transforming the data non-linearly using activation functions. The mathematics of the hidden layer can be represented as follows:\\n\\n$$y_k = g(\\\\sum_{i=1}^{n} w_{ik}x_i + b_k)$$\\n\\nwhere:\\n\\n* $y_k$ is the output of the k-th node in the hidden layer\\n* $w_{ik}$ are the weights between the input data ($x_i$) and the k-th node in the hidden layer\\n* $b_k$ is the bias term for the k-th node in the hidden layer\\n* $n$ is the number of input data points\\n\\nThe activation function $g($.) is chosen by us, and it is used to introduce non-linearity in the transformation. Some common activation functions used in neural networks include sigmoid, tanh, ReLU (Rectified Linear Unit), and softmax.\\n\\nThe hidden layer takes all the features as inputs and transforms them using the activation function. This allows the network to learn complex patterns in the data, even if the individual features are not very informative on their own.\\n\\nIn a multilayer neural network, each hidden layer functions exactly the same way as described above, with the difference being that the second hidden layer gets its input from the first hidden layer (not the input layer). This allows the network to learn more complex and abstract representations of the data as it moves through the layers.\\n\\nI hope this helps! Let me know if you have any questions or need further clarification.\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
